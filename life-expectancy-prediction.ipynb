{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective: Create a Regression Model to predict the life expectancy and understand which factors can help us better forecast the life expectancy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0) Dependencise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# data processing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "# modeling \n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, FunctionTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV, HalvingRandomSearchCV, RandomizedSearchCV\n",
    "from sklearn.feature_selection import RFE, RFECV, SelectFromModel\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import make_column_selector, make_column_transformer\n",
    "\n",
    "\n",
    "# Algorithms\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import IsolationForest, RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.linear_model import LinearRegression, LassoCV\n",
    "\n",
    "\n",
    "# metrics \n",
    "from scipy.optimize import least_squares\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, median_absolute_error, explained_variance_score, r2_score\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [10, 5] #change size of plot\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Data Acquistion\n",
    "\n",
    "A quick look at:\n",
    "1. Data Structure / Data Info\n",
    "2. Categorical Variables\n",
    "3. Numerical Variables\n",
    "\n",
    "Data Dictionary:\n",
    "- country: the country in which the indicators are from (i.e. United States of America or Congo)\n",
    "- year: the calendar year the indicators are from (ranging from 2000 to 2015)\n",
    "- status: whether a country is considered to be 'Developing' or 'Developed' by WHO standards\n",
    "- life_expectancy: the life expectancy of people in years for a particular country and year\n",
    "\n",
    "- adult_mortality (Ratio) - the adult mortality rate per 1000 population \n",
    "- infant_deaths (Ratio) - number of infant deaths per 1000 population; similar to above, but for infants\n",
    "- alcohol (Ratio) - a country's alcohol consumption rate measured as liters of pure alcohol consumption per capita\n",
    "\n",
    "- percentage_expenditure (Ratio) - expenditure on health as a percentage of Gross Domestic Product (gdp)\n",
    "- hepatitis_b (Ratio): number of 1 year olds with Hepatitis B immunization over all 1 year olds in population\n",
    "- measles (Number): number of reported Measles cases\n",
    "- BMI: average Body Mass Index (BMI) of a country's total population\n",
    "\n",
    "- under-five_deaths (Ratio) - number of people under the age of five deaths per 1000 population\n",
    "- polio (Ratio) - number of 1 year olds with Polio immunization over the number of all 1 year olds in population\n",
    "- total_expenditure (Ratio) - government expenditure on health as a percentage of total government expenditure\n",
    "- diphtheria (Ratio) - Diphtheria tetanus toxoid and pertussis (DTP3) immunization rate of 1 year olds\n",
    "- hiv/aids (Ratio) - deaths per 1000 live births caused by HIV/AIDS for people under 5; number of people under 5 who die due to HIV/AIDS per 1000 births\n",
    "\n",
    "- gdp: Gross Domestic Product per capita ($USD)\n",
    "- population: population of a country\n",
    "\n",
    "- thinness_1-19_years (Ratio) - rate of thinness among people aged 10-19 \n",
    "- thinness_5-9_years (Ratio) - rate of thinness among people aged 5-9\n",
    "- income_composition_of_resources (Ratio) - Human Development Index in terms of income composition of resources (index ranging from 0 to 1)\n",
    "\n",
    "- schooling (Number) - average number of years of schooling of a population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "life_expectancy_data = pd.read_csv(\"Life Expectancy Data.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1) A quick look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "life_expectancy_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "life_expectancy_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "life_expectancy_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim the spaces in the column names\n",
    "trimed_col = []\n",
    "for col in life_expectancy_data.columns:\n",
    "    trimed_name = col.strip()\n",
    "    trimed_name = trimed_name.replace(\"  \",\" \")\n",
    "\n",
    "    if trimed_name not in ['BMI', 'HIV/AIDS','GDP']:\n",
    "        trimed_col.append(trimed_name.title())\n",
    "    else:\n",
    "        trimed_col.append(trimed_name)\n",
    "    \n",
    "life_expectancy_data.columns = trimed_col\n",
    "\n",
    "# Note that according to the data dictionary, the first column of thinness represent\n",
    "# Rate of thinness among people aged 10-19. Hence we should rename the column from '1-19' to '10-19'.\n",
    "life_expectancy_data.rename(columns = {'Thinness 1-19 Years':'Thinness 10-19 Years'},inplace = True)\n",
    "life_expectancy_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "life_expectancy_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "life_expectancy_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_PATH = Path() / \"images\" \n",
    "IMAGES_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)\n",
    "\n",
    "life_expectancy_data.hist(bins=50, figsize=(12, 8))\n",
    "save_fig(\"attribute_histogram_plots\")  \n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) Pandas Profiling\n",
    "\n",
    "Pandas Profiling generates a global deatiled report about the dataset, including the number of records, the number of features, overall missingness and duplicates. It also includes descriptive statistics for each individual variables and correlation statistics between the variables. Refer to 'original_dataset.html' to see the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_profiling\n",
    "\n",
    "profile = life_expectancy_data.profile_report(title='Pandas Profiling Report')\n",
    "profile.to_file(\"original_dataset.html\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3) Data Correction\n",
    "\n",
    "Looking at the above data distribution, we can immediately tell that some values are off.\n",
    "\n",
    "- **Percentage Expenditure** as the current health percentage of GDP should be a percentage, it cannot go over the range of [0,100]. While in this dataset it has a range from [0,19479] and a mean of 738, this data should be wrong.\n",
    "- **BMI** as a obesity indicateor usually range from [17,30], again this data is completely off.\n",
    "- The overall distribution of **GDP** and **Population** seems valid, but they have many small values that do not make sense (for example, some countries have population < 50), and they have over 30% of the missing value.\n",
    "\n",
    "The good news is all these common information can easily be found online. To improve the data quality, I replaced these features with actual dataset retrieved from WHO, OurWorldInData, and DataWorldBank.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = life_expectancy_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop = pd.read_csv(\"Population-DataWorldBank.csv\")\n",
    "\n",
    "df = pd.merge(df, pop, how='left', on = ['Country', 'Year'])\n",
    "df = df.drop(columns = 'Population')\n",
    "df = df.rename(columns={'Value':'Population'})\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of null value did not change, it is possible that the country names are different in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Country'][df['Population'].isnull()].unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use country_converter to unify the country name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import country_converter as coco\n",
    "\n",
    "df = life_expectancy_data.copy()\n",
    "df['Country'] = coco.convert(names = df['Country'], to='name_short')\n",
    "\n",
    "df['Country'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import country_converter as coco\n",
    "\n",
    "pop = pd.read_csv(\"Population-DataWorldBank.csv\")\n",
    "pop['Country'] = coco.convert(names = pop['Country'], to='name_short')\n",
    "\n",
    "df = pd.merge(df, pop, how='left', on = ['Country', 'Year'])\n",
    "df = df.drop(columns = 'Population')\n",
    "df = df.rename(columns={'Value':'Population'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Population'].isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of null value significantly decrease! The assumption on the difference between country names was correct.\n",
    "\n",
    "Coundct the same steps for all the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GDP per capital\n",
    "gdp = pd.read_csv(\"GDP-DataWorldBank.csv\")\n",
    "gdp['Country'] = coco.convert(names = gdp['Country'], to='name_short')\n",
    "\n",
    "df = pd.merge(df, gdp, how='left', on = ['Country', 'Year'])\n",
    "df = df.drop(columns = 'GDP')\n",
    "df = df.rename(columns={'Value':'GDP'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['GDP'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current Health Expenditure (% of GDP)\n",
    "che = pd.read_csv(\"CHE-WHO.csv\")\n",
    "che['Country'] = coco.convert(names = che['Country'], to='name_short')\n",
    "\n",
    "df = pd.merge(df, che, how='left',on = ['Country', 'Year'])\n",
    "\n",
    "df = df.drop(columns = {'Percentage Expenditure','Indicator','ParentLocation'})\n",
    "df = df.rename(columns={'Value':'Percentage Expenditure'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Percentage Expenditure'].isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the number of null values increased. However, according to the panda profiiling report, there are over 20% of the values are zero, which means over 600 records were pure zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Country'][df['Percentage Expenditure'].isnull()].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Body Mass Index\n",
    "bmi = pd.read_csv(\"BMI-OurWorldInData.csv\")\n",
    "bmi['Country'] = coco.convert(names = bmi['Country'], to='name_short')\n",
    "\n",
    "df = pd.merge(df, bmi, how=\"left\", on = ['Country', 'Year'])\n",
    "df = df.drop(columns = {'BMI','Mean BMI (female)','Mean BMI (male)'})\n",
    "df = df.rename(columns= {'Mean BMI':'BMI'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BMI'].isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarily, I also found the dataset for those features that have a large number of null values or zeros:\n",
    "- Infant Deaths: 28.9% zeros        ~= 875 records\n",
    "- Under-Five Deaths: 26.7% Zeros    ~=784 records\n",
    "- Hepatitis B: 18.8% Missing Values ~=552 records\n",
    "- Measles: 33.5% zeros              ~=984 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infant Deaths\n",
    "infant = pd.read_csv(\"InfantMortalityPer1000-DataWorldBank.csv\")\n",
    "infant['Country'] = coco.convert(names = infant['Country'], to='name_short')\n",
    "\n",
    "df = pd.merge(df, infant, how='left', on = ['Country', 'Year'])\n",
    "df = df.drop(columns = 'Infant Deaths')\n",
    "df = df.rename(columns={'Value':'Infant Deaths'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Infant Deaths'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deaths Under 5\n",
    "child = pd.read_csv(\"InfantMortalityPer1000-DataWorldBank.csv\")\n",
    "child['Country'] = coco.convert(names = child['Country'], to='name_short')\n",
    "\n",
    "df = pd.merge(df, child, how='left', on = ['Country', 'Year'])\n",
    "df = df.drop(columns = 'Under-Five Deaths')\n",
    "df = df.rename(columns={'Value':'Under-Five Deaths'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Under-Five Deaths'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hep B\n",
    "hepb = pd.read_csv(\"HepB-IMU-DataWorld.csv\")\n",
    "hepb['Country'] = coco.convert(names = hepb['Country'], to='name_short')\n",
    "\n",
    "df = pd.merge(df, hepb, how='left', on = ['Country', 'Year'])\n",
    "df = df.drop(columns = 'Hepatitis B')\n",
    "df = df.rename(columns={'Value':'Hepatitis B'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Hepatitis B'].isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that we do have a lot of missing information regarding Hepatitis B Immutization rate across countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measles\n",
    "measles = pd.read_csv(\"Measles-OurWorldInData.csv\")\n",
    "measles['Country'] = coco.convert(names = measles['Country'], to='name_short')\n",
    "\n",
    "df = pd.merge(df, measles, how='left', on = ['Country', 'Year'])\n",
    "df = df.drop(columns = 'Measles')\n",
    "df = df.rename(columns={'Value':'Measles'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Measles'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('new_LifeExp.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4) A quick look at the new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)\n",
    "\n",
    "df.hist(bins=50, figsize=(12, 8))\n",
    "save_fig(\"new_attribute_histogram_plots\")  \n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many features are skewed, we might need to consider standardization/normalization when building the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5) Sample a test set (remain unseen)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.1) Method 1:  Randomly split the data using np.randodm.permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_and_split_data(data, test_ratio):\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    return data.iloc[train_indices], data.iloc[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "train_set, test_set = shuffle_and_split_data(df, 0.2)\n",
    "\n",
    "print(len(train_set),len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.2) Method 2: Using hash() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zlib import crc32\n",
    "\n",
    "def is_id_in_test_set(identifier, test_ratio):\n",
    "    return crc32(np.int64(identifier)) < test_ratio * 2**32\n",
    "\n",
    "def split_data_with_id_hash(data, test_ratio, id_column):\n",
    "    ids = data[id_column]\n",
    "    in_test_set = ids.apply(lambda id_: is_id_in_test_set(id_, test_ratio))\n",
    "    return data.loc[~in_test_set], data.loc[in_test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_id = df.reset_index()  # adds an `index` column\n",
    "train_set, test_set = split_data_with_id_hash(df_with_id, 0.2, \"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_set),len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.3) Method 3: Using sklearn model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(df, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_set),len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Data Exploration\n",
    "\n",
    "Again I used panda profiling to explore the following:\n",
    "- Descriptive Statistics\n",
    "- Missing Data (%)\n",
    "- Duplicates (%)\n",
    "- Correlation\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1) Pandas Profiling\n",
    "\n",
    "Refer to 'output.html' to see the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_profiling\n",
    "\n",
    "profile = train_set.profile_report(title='Pandas Profiling Report')\n",
    "profile.to_file(\"output.html\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Correlation HeatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = train_set.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix['Life Expectancy'].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there is any collinearity between variables\n",
    "plt.figure(figsize=(26,6))\n",
    "heatmap = sns.heatmap(train_set.corr(), vmin=-1, vmax=1, annot=True, cmap='BrBG')\n",
    "heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':18}, pad=12)\n",
    "\n",
    "path = IMAGES_PATH / 'heatmap.png'\n",
    "plt.savefig(path, dpi=300, bbox_inches='tight')\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further explore the relationship between LifeExpectancy and each independent variable, certain data cleaning needs to be done."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Splitting Training set and Validation Set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to split the dataset into training set and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set, validation_set = train_test_split(train_set, test_size = 0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(training_set),len(validation_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Building a baseline Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a simple baseline model, select only numeric features with less than 5% missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_train = training_set[['Life Expectancy', 'Adult Mortality',\n",
    "       'Alcohol', 'Polio', 'Diphtheria', 'HIV/AIDS',\n",
    "       'Thinness 10-19 Years', 'Thinness 5-9 Years',\n",
    "       'Income Composition Of Resources', 'Schooling', 'Population', 'GDP',\n",
    "       'Percentage Expenditure', 'BMI', 'Infant Deaths', 'Under-Five Deaths']].dropna()\n",
    "\n",
    "baseline_valid = validation_set[['Life Expectancy', 'Adult Mortality',\n",
    "       'Alcohol', 'Polio', 'Diphtheria', 'HIV/AIDS',\n",
    "       'Thinness 10-19 Years', 'Thinness 5-9 Years',\n",
    "       'Income Composition Of Resources', 'Schooling', 'Population', 'GDP',\n",
    "       'Percentage Expenditure', 'BMI', 'Infant Deaths', 'Under-Five Deaths']].dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = baseline_train.drop(columns='Life Expectancy')\n",
    "y_train = baseline_train['Life Expectancy']\n",
    "\n",
    "X_valid = baseline_valid.drop(columns='Life Expectancy')\n",
    "y_valid = baseline_valid['Life Expectancy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = DummyRegressor(strategy=\"mean\")\n",
    "dummy.fit(X_train,y_train)\n",
    "y_pred = dummy.predict(X_valid)\n",
    "\n",
    "mean_squared_error(y_pred, y_valid, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-cross_val_score(dummy, X_train, y_train,\n",
    "                scoring=\"neg_root_mean_squared_error\", cv=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Data Cleaning\n",
    "\n",
    "1. Removing correlated variables \n",
    "2. Dealing with Missing data\n",
    "3. Outlier Detection\n",
    "4. Feature engineering (Categorical)\n",
    "5. Feature Scaling\n",
    "6. Feature Selection\n",
    "\n",
    "\n",
    "7. Clustering\n",
    "9. Imbalanced data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1) Removing Correlated Variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the heatmap, the following pair of variables are highly correlated:\n",
    "- \"Thinness 10-19 Years\" and \"Thinness 5-9 Years\"\n",
    "- \"Infant Deaths\" and \"Under-Five Deaths\"\n",
    "- 'Income Composition of Resources\" and \"Schooling\"\n",
    "\n",
    "For each pair, drop the one that is less correlated with 'Life Expectancy'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = training_set.drop(columns={'Thinness 10-19 Years','Under-Five Deaths','Schooling'})\n",
    "training_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2) Dealing with Missing Values\n",
    "\n",
    "We consider three options for handling the missing values.\n",
    "1. Drop the records that contain missing values\n",
    "2. Drop the columns that contain missing values\n",
    "3. Impute the missing values with mean/median/mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify percentage of null values in each column. (Can also be found in output.html)\n",
    "training_set.isnull().sum()*100/training_set.isnull().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_rows_idx = training_set.isnull().any(axis=1)\n",
    "training_set.loc[null_rows_idx].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.loc[null_rows_idx].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing value\n",
    "le_option1 = training_set.copy()\n",
    "le_option1.dropna(inplace=True)\n",
    "le_option1.loc[null_rows_idx].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns withh missing value\n",
    "le_option2 = training_set.copy()\n",
    "le_option2.dropna(axis='columns',inplace=True)  # option 2\n",
    "le_option2.loc[null_rows_idx].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute the missing values with statistical measures\n",
    "le_option3 = training_set.copy()\n",
    "le_option3.fillna(le_option3.mean(), inplace=True)  # or median() / mode()\n",
    "le_option3.loc[null_rows_idx].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hepatitis B has almost 20% missing value, simply dropping the records will result in huge data loss, therefore we abandon option1.\n",
    "\n",
    "To see if we can apply option2, check the relationship between Hepatitis B and Life Expectancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix['Life Expectancy']['Hepatitis B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Hepatitis B Immunization Rate over Years\n",
    "training_set.groupby('Year')['Hepatitis B'].mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set['Hepatitis B'][training_set['Year']==2003].isnull().count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like there is a lot of value missing in the year of 2003. But in general it does show a linear relationship between Hepatitis B and Life Expectancy.\n",
    "\n",
    "Move on to option 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.groupby('Year')['Hepatitis B'].median().plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the same strategy to all columns with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating out the numerical attributes to use the \"median\" strategy\n",
    "training_set_num = training_set.select_dtypes(include=[np.number])\n",
    "\n",
    "imputer.fit(training_set_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify if imputer is working well\n",
    "training_set_num.median().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = imputer.transform(training_set_num)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add the imputed value back to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set[imputer.feature_names_in_] = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.loc[null_rows_idx].head()\n",
    "\n",
    "training_set.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3) Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isolation_forest = IsolationForest(random_state=42)\n",
    "outlier_pred = isolation_forest.fit_predict(X)\n",
    "\n",
    "anomaly_values = training_set.iloc[outlier_pred == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_values.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to drop outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#le = le.iloc[outlier_pred == 1]\n",
    "#le.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4) Feature Engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Categorical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set['Country'].value_counts().plot(kind='bar', fontsize=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Country has a lot of unique value, using one-hot encodinig would result in a high dimensionality and could lead to overfitting. \n",
    "\n",
    "We could consider other encoding techniques such as Label Encoding. However, considering that we already have features like 'GDP per capital' and 'Population', it is not really necessary to include 'Country' becuase GDP and population likely capture much of the information that 'Country' can provide.\n",
    "\n",
    "Therefore, simply drop this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = training_set.drop(columns='Country')\n",
    "training_set.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset only contains two different type of country status, therefore we can apply label encoing by assigning a value of 0 to one category and 1 to the other cateogry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set['Status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set['Status'] = LabelEncoder().fit_transform(training_set['Status'])\n",
    "training_set['Status'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5) Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = training_set.loc[:,training_set.columns != 'Life Expectancy']\n",
    "y = training_set['Life Expectancy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "le_num_min_max_scaled = min_max_scaler.fit_transform(training_set_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scaler = StandardScaler()\n",
    "le_num_std_scaled = std_scaler.fit_transform(training_set_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_std = std_scaler.fit_transform(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6) Feature Selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First use RFECV to determine the optimal number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg =  LinearRegression()\n",
    "selector = RFECV(estimator=reg,cv=5)\n",
    "selector.fit(X_std,y)\n",
    "X_new = selector.transform(X_std)\n",
    "\n",
    "X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector.ranking_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector.n_features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = X.columns[selector.support_]\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[selected_features]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use LASSO to verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = LassoCV().fit(X_std, y)\n",
    "\n",
    "importance_features = lasso.coef_ !=0\n",
    "importance_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns[importance_features]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Transformation Pipelines\n",
    "\n",
    "Organize all data preprocessing in step 3 into one pipeline, including:\n",
    "1. Removing correlated variables\n",
    "2. Imputing missing values\n",
    "3. Handling categorical variables\n",
    "4. Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_drop = ['Country','Thinness 10-19 Years','Under-Five Deaths','Schooling']\n",
    "\n",
    "train_set = train_set.drop(columns=col_to_drop)\n",
    "test_set = test_set.drop(columns=col_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation and Standardization\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"standardize\", StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"label_encoder\", FunctionTransformer(LabelEncoder().fit_transform)),\n",
    "    (\"reshape\", FunctionTransformer(lambda x: x.reshape(-1,1), validate=False))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the preprocessing pipeline\n",
    "preprocessing = make_column_transformer(\n",
    "    (num_pipeline, make_column_selector(dtype_include=np.number)),\n",
    "    (cat_pipeline, make_column_selector(dtype_include=object))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = train_set.drop(columns={'Life Expectancy'})\n",
    "\n",
    "le_prepared = preprocessing.fit_transform(test)\n",
    "le_prepared[:2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Modelling and Evaluating on Traing Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set, validation_set = train_test_split(train_set, test_size = 0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = training_set.loc[:,training_set.columns != 'Life Expectancy']\n",
    "y_train = imputer.fit_transform(training_set[['Life Expectancy']])\n",
    "\n",
    "X_valid = validation_set.loc[:,validation_set.columns != 'Life Expectancy']\n",
    "y_valid = imputer.fit_transform(validation_set[['Life Expectancy']])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1） Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = make_pipeline(preprocessing, LinearRegression())\n",
    "lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_pred = lr.predict(X_valid) \n",
    "\n",
    "print(\"Mean absolute error =\", round(mean_absolute_error(y_valid, y_valid_pred), 2)) \n",
    "print(\"Mean squared error =\", round(mean_squared_error(y_valid, y_valid_pred), 2)) \n",
    "print(\"Median absolute error =\", round(median_absolute_error(y_valid, y_valid_pred), 2)) \n",
    "print(\"Explain variance score =\", round(explained_variance_score(y_valid, y_valid_pred), 2)) \n",
    "print(\"R2 score =\", round(r2_score(y_valid, y_valid_pred), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(lr, X=X_train, y=y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_rmses = -cross_val_score(lr, X_train, y_train,\n",
    "                              scoring=\"neg_root_mean_squared_error\", cv=10)\n",
    "pd.Series(lr_rmses).describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2) Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_reg = make_pipeline(preprocessing, DecisionTreeRegressor(random_state=333))\n",
    "tree_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_pred = tree_reg.predict(X_valid)\n",
    "\n",
    "print(\"Mean absolute error =\", round(mean_absolute_error(y_valid, y_valid_pred), 2)) \n",
    "print(\"Mean squared error =\", round(mean_squared_error(y_valid, y_valid_pred), 2)) \n",
    "print(\"Median absolute error =\", round(median_absolute_error(y_valid, y_valid_pred), 2)) \n",
    "print(\"Explain variance score =\", round(explained_variance_score(y_valid, y_valid_pred), 2)) \n",
    "print(\"R2 score =\", round(r2_score(y_valid, y_valid_pred), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(tree_reg, X=X_train, y=y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_base_rmses = -cross_val_score(tree_reg, X_train, y_train,\n",
    "                              scoring=\"neg_root_mean_squared_error\", cv=10)\n",
    "pd.Series(tree_base_rmses).describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Regressor performs better than linear regression model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3) Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_reg = make_pipeline(preprocessing,\n",
    "                           RandomForestRegressor(random_state=33))\n",
    "forest_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_pred = forest_reg.predict(X_valid)\n",
    "\n",
    "print(\"Mean absolute error =\", round(mean_absolute_error(y_valid, y_valid_pred), 2)) \n",
    "print(\"Mean squared error =\", round(mean_squared_error(y_valid, y_valid_pred), 2)) \n",
    "print(\"Median absolute error =\", round(median_absolute_error(y_valid, y_valid_pred), 2)) \n",
    "print(\"Explain variance score =\", round(explained_variance_score(y_valid, y_valid_pred), 2)) \n",
    "print(\"R2 score =\", round(r2_score(y_valid, y_valid_pred), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(forest_reg, X=X_train, y=y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_rmses = -cross_val_score(forest_reg, X_train, y_train,\n",
    "                              scoring=\"neg_root_mean_squared_error\", cv=10)\n",
    "pd.Series(forest_rmses).describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even better!!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try reduce the dimensions and use the features selected before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_selected = X_train[selected_features]\n",
    "X_valid_selected = X_valid[selected_features]\n",
    "\n",
    "X_train_selected.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_selected = make_pipeline(preprocessing, RandomForestRegressor())\n",
    "forest_selected.fit(X_train_selected,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_pred = forest_selected.predict(X_valid_selected) \n",
    "\n",
    "print(\"Mean absolute error =\", round(mean_absolute_error(y_valid, y_valid_pred), 2)) \n",
    "print(\"Mean squared error =\", round(mean_squared_error(y_valid, y_valid_pred), 2)) \n",
    "print(\"Median absolute error =\", round(median_absolute_error(y_valid, y_valid_pred), 2)) \n",
    "print(\"Explain variance score =\", round(explained_variance_score(y_valid, y_valid_pred), 2)) \n",
    "print(\"R2 score =\", round(r2_score(y_valid, y_valid_pred), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(forest_selected, X=X_train_selected, y=y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_selected_rmses = -cross_val_score(forest_selected, X_train_selected, y_train,\n",
    "                              scoring=\"neg_root_mean_squared_error\", cv=10)\n",
    "pd.Series(lr_selected_rmses).describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With less feature, the performance of the model remains almost the same. Base on the principle of Occam's razor, the simpler the better. Therefore we shall proceed with less features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_selected\n",
    "X_valid = X_valid_selected"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7) Fine Tune the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1) Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-27 {color: black;background-color: white;}#sk-container-id-27 pre{padding: 0;}#sk-container-id-27 div.sk-toggleable {background-color: white;}#sk-container-id-27 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-27 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-27 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-27 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-27 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-27 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-27 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-27 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-27 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-27 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-27 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-27 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-27 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-27 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-27 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-27 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-27 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-27 div.sk-item {position: relative;z-index: 1;}#sk-container-id-27 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-27 div.sk-item::before, #sk-container-id-27 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-27 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-27 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-27 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-27 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-27 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-27 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-27 div.sk-label-container {text-align: center;}#sk-container-id-27 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-27 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-27\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[(&#x27;preprocessing&#x27;,\n",
       "                                        ColumnTransformer(transformers=[(&#x27;pipeline-1&#x27;,\n",
       "                                                                         Pipeline(steps=[(&#x27;impute&#x27;,\n",
       "                                                                                          SimpleImputer(strategy=&#x27;median&#x27;)),\n",
       "                                                                                         (&#x27;standardize&#x27;,\n",
       "                                                                                          StandardScaler())]),\n",
       "                                                                         &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001E5F0037850&gt;),\n",
       "                                                                        (&#x27;pipeline-2&#x27;,\n",
       "                                                                         Pipeline(steps=[(&#x27;impute&#x27;,\n",
       "                                                                                          SimpleImputer(strategy...\n",
       "                                                                                         (&#x27;reshape&#x27;,\n",
       "                                                                                          FunctionTransformer(func=&lt;function &lt;lambda&gt; at 0x000001E6171170A0&gt;))]),\n",
       "                                                                         &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001E616F641F0&gt;)])),\n",
       "                                       (&#x27;random_forest&#x27;,\n",
       "                                        RandomForestRegressor(random_state=33))]),\n",
       "             param_grid=[{&#x27;random_forest__max_features&#x27;: [4, 6, 8]},\n",
       "                         {&#x27;random_forest__max_features&#x27;: [6, 8, 10]}],\n",
       "             scoring=&#x27;neg_root_mean_squared_error&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-238\" type=\"checkbox\" ><label for=\"sk-estimator-id-238\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[(&#x27;preprocessing&#x27;,\n",
       "                                        ColumnTransformer(transformers=[(&#x27;pipeline-1&#x27;,\n",
       "                                                                         Pipeline(steps=[(&#x27;impute&#x27;,\n",
       "                                                                                          SimpleImputer(strategy=&#x27;median&#x27;)),\n",
       "                                                                                         (&#x27;standardize&#x27;,\n",
       "                                                                                          StandardScaler())]),\n",
       "                                                                         &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001E5F0037850&gt;),\n",
       "                                                                        (&#x27;pipeline-2&#x27;,\n",
       "                                                                         Pipeline(steps=[(&#x27;impute&#x27;,\n",
       "                                                                                          SimpleImputer(strategy...\n",
       "                                                                                         (&#x27;reshape&#x27;,\n",
       "                                                                                          FunctionTransformer(func=&lt;function &lt;lambda&gt; at 0x000001E6171170A0&gt;))]),\n",
       "                                                                         &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001E616F641F0&gt;)])),\n",
       "                                       (&#x27;random_forest&#x27;,\n",
       "                                        RandomForestRegressor(random_state=33))]),\n",
       "             param_grid=[{&#x27;random_forest__max_features&#x27;: [4, 6, 8]},\n",
       "                         {&#x27;random_forest__max_features&#x27;: [6, 8, 10]}],\n",
       "             scoring=&#x27;neg_root_mean_squared_error&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-239\" type=\"checkbox\" ><label for=\"sk-estimator-id-239\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessing&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;pipeline-1&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;impute&#x27;,\n",
       "                                                                   SimpleImputer(strategy=&#x27;median&#x27;)),\n",
       "                                                                  (&#x27;standardize&#x27;,\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001E5F0037850&gt;),\n",
       "                                                 (&#x27;pipeline-2&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;impute&#x27;,\n",
       "                                                                   SimpleImputer(strategy=&#x27;most_frequent&#x27;)),\n",
       "                                                                  (&#x27;label_encoder&#x27;,\n",
       "                                                                   FunctionTransformer(func=&lt;bound method LabelEncoder.fit_transform of LabelEncoder()&gt;)),\n",
       "                                                                  (&#x27;reshape&#x27;,\n",
       "                                                                   FunctionTransformer(func=&lt;function &lt;lambda&gt; at 0x000001E6171170A0&gt;))]),\n",
       "                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001E616F641F0&gt;)])),\n",
       "                (&#x27;random_forest&#x27;, RandomForestRegressor(random_state=33))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-240\" type=\"checkbox\" ><label for=\"sk-estimator-id-240\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessing: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;pipeline-1&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;impute&#x27;,\n",
       "                                                  SimpleImputer(strategy=&#x27;median&#x27;)),\n",
       "                                                 (&#x27;standardize&#x27;,\n",
       "                                                  StandardScaler())]),\n",
       "                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001E5F0037850&gt;),\n",
       "                                (&#x27;pipeline-2&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;impute&#x27;,\n",
       "                                                  SimpleImputer(strategy=&#x27;most_frequent&#x27;)),\n",
       "                                                 (&#x27;label_encoder&#x27;,\n",
       "                                                  FunctionTransformer(func=&lt;bound method LabelEncoder.fit_transform of LabelEncoder()&gt;)),\n",
       "                                                 (&#x27;reshape&#x27;,\n",
       "                                                  FunctionTransformer(func=&lt;function &lt;lambda&gt; at 0x000001E6171170A0&gt;))]),\n",
       "                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001E616F641F0&gt;)])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-241\" type=\"checkbox\" ><label for=\"sk-estimator-id-241\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">pipeline-1</label><div class=\"sk-toggleable__content\"><pre>&lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001E5F0037850&gt;</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-242\" type=\"checkbox\" ><label for=\"sk-estimator-id-242\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(strategy=&#x27;median&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-243\" type=\"checkbox\" ><label for=\"sk-estimator-id-243\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-244\" type=\"checkbox\" ><label for=\"sk-estimator-id-244\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">pipeline-2</label><div class=\"sk-toggleable__content\"><pre>&lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001E616F641F0&gt;</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-245\" type=\"checkbox\" ><label for=\"sk-estimator-id-245\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(strategy=&#x27;most_frequent&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-246\" type=\"checkbox\" ><label for=\"sk-estimator-id-246\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">FunctionTransformer</label><div class=\"sk-toggleable__content\"><pre>FunctionTransformer(func=&lt;bound method LabelEncoder.fit_transform of LabelEncoder()&gt;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-247\" type=\"checkbox\" ><label for=\"sk-estimator-id-247\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">FunctionTransformer</label><div class=\"sk-toggleable__content\"><pre>FunctionTransformer(func=&lt;function &lt;lambda&gt; at 0x000001E6171170A0&gt;)</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-248\" type=\"checkbox\" ><label for=\"sk-estimator-id-248\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(random_state=33)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[('preprocessing',\n",
       "                                        ColumnTransformer(transformers=[('pipeline-1',\n",
       "                                                                         Pipeline(steps=[('impute',\n",
       "                                                                                          SimpleImputer(strategy='median')),\n",
       "                                                                                         ('standardize',\n",
       "                                                                                          StandardScaler())]),\n",
       "                                                                         <sklearn.compose._column_transformer.make_column_selector object at 0x000001E5F0037850>),\n",
       "                                                                        ('pipeline-2',\n",
       "                                                                         Pipeline(steps=[('impute',\n",
       "                                                                                          SimpleImputer(strategy...\n",
       "                                                                                         ('reshape',\n",
       "                                                                                          FunctionTransformer(func=<function <lambda> at 0x000001E6171170A0>))]),\n",
       "                                                                         <sklearn.compose._column_transformer.make_column_selector object at 0x000001E616F641F0>)])),\n",
       "                                       ('random_forest',\n",
       "                                        RandomForestRegressor(random_state=33))]),\n",
       "             param_grid=[{'random_forest__max_features': [4, 6, 8]},\n",
       "                         {'random_forest__max_features': [6, 8, 10]}],\n",
       "             scoring='neg_root_mean_squared_error')"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_pipeline = Pipeline([\n",
    "    (\"preprocessing\", preprocessing),\n",
    "    (\"random_forest\", RandomForestRegressor(random_state=33)),\n",
    "])\n",
    "param_grid = [\n",
    "    {'random_forest__max_features': [4, 6, 8]},\n",
    "    {'random_forest__max_features': [6, 8, 10]},\n",
    "]\n",
    "grid_search = GridSearchCV(full_pipeline, param_grid, cv=3,\n",
    "                           scoring='neg_root_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_res = pd.DataFrame(grid_search.cv_results_)\n",
    "cv_res.sort_values(by=\"mean_test_score\", ascending=False, inplace=True)\n",
    "\n",
    "# extra code – these few lines of code just make the DataFrame look nicer\n",
    "cv_res = cv_res[[\"param_random_forest__max_features\", \"split0_test_score\",\n",
    "                 \"split1_test_score\", \"split2_test_score\", \"mean_test_score\"]]\n",
    "score_cols = [\"split0\", \"split1\", \"split2\", \"mean_test_rmse\"]\n",
    "cv_res.columns = [ \"max_features\"] + score_cols\n",
    "cv_res[score_cols] = -cv_res[score_cols].astype(np.float64)\n",
    "\n",
    "cv_res.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2) Randomized Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-26 {color: black;background-color: white;}#sk-container-id-26 pre{padding: 0;}#sk-container-id-26 div.sk-toggleable {background-color: white;}#sk-container-id-26 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-26 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-26 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-26 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-26 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-26 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-26 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-26 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-26 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-26 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-26 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-26 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-26 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-26 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-26 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-26 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-26 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-26 div.sk-item {position: relative;z-index: 1;}#sk-container-id-26 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-26 div.sk-item::before, #sk-container-id-26 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-26 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-26 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-26 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-26 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-26 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-26 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-26 div.sk-label-container {text-align: center;}#sk-container-id-26 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-26 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-26\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=3,\n",
       "                   estimator=Pipeline(steps=[(&#x27;preprocessing&#x27;,\n",
       "                                              ColumnTransformer(transformers=[(&#x27;pipeline-1&#x27;,\n",
       "                                                                               Pipeline(steps=[(&#x27;impute&#x27;,\n",
       "                                                                                                SimpleImputer(strategy=&#x27;median&#x27;)),\n",
       "                                                                                               (&#x27;standardize&#x27;,\n",
       "                                                                                                StandardScaler())]),\n",
       "                                                                               &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001E5F0037850&gt;),\n",
       "                                                                              (&#x27;pipeline-2&#x27;,\n",
       "                                                                               Pipeline(steps=[(&#x27;impute&#x27;,\n",
       "                                                                                                SimpleImputer(st...\n",
       "                                                                                                FunctionTransformer(func=&lt;function &lt;lambda&gt; at 0x000001E6171170A0&gt;))]),\n",
       "                                                                               &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001E616F641F0&gt;)])),\n",
       "                                             (&#x27;random_forest&#x27;,\n",
       "                                              RandomForestRegressor(random_state=33))]),\n",
       "                   param_distributions={&#x27;random_forest__max_features&#x27;: &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000001E5D7474550&gt;},\n",
       "                   random_state=42, scoring=&#x27;neg_root_mean_squared_error&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-227\" type=\"checkbox\" ><label for=\"sk-estimator-id-227\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=3,\n",
       "                   estimator=Pipeline(steps=[(&#x27;preprocessing&#x27;,\n",
       "                                              ColumnTransformer(transformers=[(&#x27;pipeline-1&#x27;,\n",
       "                                                                               Pipeline(steps=[(&#x27;impute&#x27;,\n",
       "                                                                                                SimpleImputer(strategy=&#x27;median&#x27;)),\n",
       "                                                                                               (&#x27;standardize&#x27;,\n",
       "                                                                                                StandardScaler())]),\n",
       "                                                                               &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001E5F0037850&gt;),\n",
       "                                                                              (&#x27;pipeline-2&#x27;,\n",
       "                                                                               Pipeline(steps=[(&#x27;impute&#x27;,\n",
       "                                                                                                SimpleImputer(st...\n",
       "                                                                                                FunctionTransformer(func=&lt;function &lt;lambda&gt; at 0x000001E6171170A0&gt;))]),\n",
       "                                                                               &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001E616F641F0&gt;)])),\n",
       "                                             (&#x27;random_forest&#x27;,\n",
       "                                              RandomForestRegressor(random_state=33))]),\n",
       "                   param_distributions={&#x27;random_forest__max_features&#x27;: &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000001E5D7474550&gt;},\n",
       "                   random_state=42, scoring=&#x27;neg_root_mean_squared_error&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-228\" type=\"checkbox\" ><label for=\"sk-estimator-id-228\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessing&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;pipeline-1&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;impute&#x27;,\n",
       "                                                                   SimpleImputer(strategy=&#x27;median&#x27;)),\n",
       "                                                                  (&#x27;standardize&#x27;,\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001E5F0037850&gt;),\n",
       "                                                 (&#x27;pipeline-2&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;impute&#x27;,\n",
       "                                                                   SimpleImputer(strategy=&#x27;most_frequent&#x27;)),\n",
       "                                                                  (&#x27;label_encoder&#x27;,\n",
       "                                                                   FunctionTransformer(func=&lt;bound method LabelEncoder.fit_transform of LabelEncoder()&gt;)),\n",
       "                                                                  (&#x27;reshape&#x27;,\n",
       "                                                                   FunctionTransformer(func=&lt;function &lt;lambda&gt; at 0x000001E6171170A0&gt;))]),\n",
       "                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001E616F641F0&gt;)])),\n",
       "                (&#x27;random_forest&#x27;, RandomForestRegressor(random_state=33))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-229\" type=\"checkbox\" ><label for=\"sk-estimator-id-229\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessing: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;pipeline-1&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;impute&#x27;,\n",
       "                                                  SimpleImputer(strategy=&#x27;median&#x27;)),\n",
       "                                                 (&#x27;standardize&#x27;,\n",
       "                                                  StandardScaler())]),\n",
       "                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001E5F0037850&gt;),\n",
       "                                (&#x27;pipeline-2&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;impute&#x27;,\n",
       "                                                  SimpleImputer(strategy=&#x27;most_frequent&#x27;)),\n",
       "                                                 (&#x27;label_encoder&#x27;,\n",
       "                                                  FunctionTransformer(func=&lt;bound method LabelEncoder.fit_transform of LabelEncoder()&gt;)),\n",
       "                                                 (&#x27;reshape&#x27;,\n",
       "                                                  FunctionTransformer(func=&lt;function &lt;lambda&gt; at 0x000001E6171170A0&gt;))]),\n",
       "                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001E616F641F0&gt;)])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-230\" type=\"checkbox\" ><label for=\"sk-estimator-id-230\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">pipeline-1</label><div class=\"sk-toggleable__content\"><pre>&lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001E5F0037850&gt;</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-231\" type=\"checkbox\" ><label for=\"sk-estimator-id-231\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(strategy=&#x27;median&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-232\" type=\"checkbox\" ><label for=\"sk-estimator-id-232\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-233\" type=\"checkbox\" ><label for=\"sk-estimator-id-233\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">pipeline-2</label><div class=\"sk-toggleable__content\"><pre>&lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001E616F641F0&gt;</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-234\" type=\"checkbox\" ><label for=\"sk-estimator-id-234\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(strategy=&#x27;most_frequent&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-235\" type=\"checkbox\" ><label for=\"sk-estimator-id-235\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">FunctionTransformer</label><div class=\"sk-toggleable__content\"><pre>FunctionTransformer(func=&lt;bound method LabelEncoder.fit_transform of LabelEncoder()&gt;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-236\" type=\"checkbox\" ><label for=\"sk-estimator-id-236\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">FunctionTransformer</label><div class=\"sk-toggleable__content\"><pre>FunctionTransformer(func=&lt;function &lt;lambda&gt; at 0x000001E6171170A0&gt;)</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-237\" type=\"checkbox\" ><label for=\"sk-estimator-id-237\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(random_state=33)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=Pipeline(steps=[('preprocessing',\n",
       "                                              ColumnTransformer(transformers=[('pipeline-1',\n",
       "                                                                               Pipeline(steps=[('impute',\n",
       "                                                                                                SimpleImputer(strategy='median')),\n",
       "                                                                                               ('standardize',\n",
       "                                                                                                StandardScaler())]),\n",
       "                                                                               <sklearn.compose._column_transformer.make_column_selector object at 0x000001E5F0037850>),\n",
       "                                                                              ('pipeline-2',\n",
       "                                                                               Pipeline(steps=[('impute',\n",
       "                                                                                                SimpleImputer(st...\n",
       "                                                                                                FunctionTransformer(func=<function <lambda> at 0x000001E6171170A0>))]),\n",
       "                                                                               <sklearn.compose._column_transformer.make_column_selector object at 0x000001E616F641F0>)])),\n",
       "                                             ('random_forest',\n",
       "                                              RandomForestRegressor(random_state=33))]),\n",
       "                   param_distributions={'random_forest__max_features': <scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000001E5D7474550>},\n",
       "                   random_state=42, scoring='neg_root_mean_squared_error')"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_distribs = {'random_forest__max_features': randint(low=2, high=20)}\n",
    "\n",
    "rnd_search = RandomizedSearchCV(\n",
    "    full_pipeline, param_distributions=param_distribs, n_iter=10, cv=3,\n",
    "    scoring='neg_root_mean_squared_error', random_state=42)\n",
    "\n",
    "rnd_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_res = pd.DataFrame(rnd_search.cv_results_)\n",
    "cv_res.sort_values(by=\"mean_test_score\", ascending=False, inplace=True)\n",
    "cv_res = cv_res[[\"param_random_forest__max_features\", \"split0_test_score\",\n",
    "                 \"split1_test_score\", \"split2_test_score\", \"mean_test_score\"]]\n",
    "cv_res.columns = [\"max_features\"] + score_cols\n",
    "cv_res[score_cols] = -cv_res[score_cols].astype(np.float64)\n",
    "cv_res.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the best models and their errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = rnd_search.best_estimator_  # includes preprocessing\n",
    "feature_importances = final_model[\"random_forest\"].feature_importances_\n",
    "feature_importances"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8) Evaluate the model on the Unseen Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = train_set.loc[:,train_set.columns != 'Life Expectancy']\n",
    "y_test = imputer.fit_transform(train_set[['Life Expectancy']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = final_model.predict(X_test)\n",
    "\n",
    "final_rmse = mean_squared_error(y_test, y_test_pred, squared=False)\n",
    "print(final_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean absolute error =\", round(mean_absolute_error(y_test, y_test_pred), 2)) \n",
    "print(\"Mean squared error =\", round(mean_squared_error(y_test, y_test_pred), 2)) \n",
    "print(\"Median absolute error =\", round(median_absolute_error(y_test, y_test_pred), 2)) \n",
    "print(\"Explain variance score =\", round(explained_variance_score(y_test, y_test_pred), 2)) \n",
    "print(\"R2 score =\", round(r2_score(y_test, y_test_pred), 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eb8d3085d426e5e8dee3252ffbadecffc2a8c816985f9d79249a1de42c265c0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
